diff --git a/cmd/erasure-common.go b/cmd/erasure-common.go
index 1cc32d08d..f413138e4 100644
--- a/cmd/erasure-common.go
+++ b/cmd/erasure-common.go
@@ -153,8 +153,6 @@ func readMultipleFiles(ctx context.Context, disks []StorageAPI, req ReadMultiple
 		errFileVersionNotFound,
 		io.ErrUnexpectedEOF, // some times we would read without locks, ignore these errors
 		io.EOF,              // some times we would read without locks, ignore these errors
-		context.DeadlineExceeded,
-		context.Canceled,
 	}
 	ignoredErrs = append(ignoredErrs, objectOpIgnoredErrs...)
 
diff --git a/cmd/erasure-metadata.go b/cmd/erasure-metadata.go
index 42c503a95..dcdfee994 100644
--- a/cmd/erasure-metadata.go
+++ b/cmd/erasure-metadata.go
@@ -403,7 +403,7 @@ func writeUniqueFileInfo(ctx context.Context, disks []StorageAPI, origbucket, bu
 			if fi.IsValid() {
 				return disks[index].WriteMetadata(ctx, origbucket, bucket, prefix, fi)
 			}
-			return errFileCorrupt
+			return errCorruptedFormat
 		}, index)
 	}
 
diff --git a/cmd/erasure-multipart.go b/cmd/erasure-multipart.go
index 536b0f3db..d888f8d92 100644
--- a/cmd/erasure-multipart.go
+++ b/cmd/erasure-multipart.go
@@ -739,15 +739,6 @@ func (er erasureObjects) PutObjectPart(ctx context.Context, bucket, object, uplo
 	partPath := pathJoin(uploadIDPath, fi.DataDir, partSuffix)
 	onlineDisks, err = renamePart(ctx, onlineDisks, minioMetaTmpBucket, tmpPartPath, minioMetaMultipartBucket, partPath, writeQuorum)
 	if err != nil {
-		if errors.Is(err, errFileNotFound) {
-			// An in-quorum errFileNotFound means that client stream
-			// prematurely closed and we do not find any xl.meta or
-			// part.1's - in such a scenario we must return as if client
-			// disconnected. This means that erasure.Encode() CreateFile()
-			// did not do anything.
-			return pi, IncompleteBody{Bucket: bucket, Object: object}
-		}
-
 		return pi, toObjectErr(err, minioMetaMultipartBucket, partPath)
 	}
 
@@ -1323,11 +1314,11 @@ func (er erasureObjects) CompleteMultipartUpload(ctx context.Context, bucket str
 	onlineDisks, versions, oldDataDir, err := renameData(ctx, onlineDisks, minioMetaMultipartBucket, uploadIDPath,
 		partsMetadata, bucket, object, writeQuorum)
 	if err != nil {
-		return oi, toObjectErr(err, bucket, object, uploadID)
+		return oi, toObjectErr(err, bucket, object)
 	}
 
 	if err = er.commitRenameDataDir(ctx, bucket, object, oldDataDir, onlineDisks); err != nil {
-		return ObjectInfo{}, toObjectErr(err, bucket, object, uploadID)
+		return ObjectInfo{}, toObjectErr(err, bucket, object)
 	}
 
 	if !opts.Speedtest && len(versions) > 0 {
diff --git a/cmd/erasure-object.go b/cmd/erasure-object.go
index 1195c03d5..ade16abd1 100644
--- a/cmd/erasure-object.go
+++ b/cmd/erasure-object.go
@@ -562,7 +562,7 @@ func (er erasureObjects) deleteIfDangling(ctx context.Context, bucket, object st
 func fileInfoFromRaw(ri RawFileInfo, bucket, object string, readData, inclFreeVers, allParts bool) (FileInfo, error) {
 	var xl xlMetaV2
 	if err := xl.LoadOrConvert(ri.Buf); err != nil {
-		return FileInfo{}, errFileCorrupt
+		return FileInfo{}, err
 	}
 
 	fi, err := xl.ToFileInfo(bucket, object, "", inclFreeVers, allParts)
@@ -571,7 +571,7 @@ func fileInfoFromRaw(ri RawFileInfo, bucket, object string, readData, inclFreeVe
 	}
 
 	if !fi.IsValid() {
-		return FileInfo{}, errFileCorrupt
+		return FileInfo{}, errCorruptedFormat
 	}
 
 	versionID := fi.VersionID
@@ -661,7 +661,7 @@ func pickLatestQuorumFilesInfo(ctx context.Context, rawFileInfos []RawFileInfo,
 	if !lfi.IsValid() {
 		for i := range errs {
 			if errs[i] == nil {
-				errs[i] = errFileCorrupt
+				errs[i] = errCorruptedFormat
 			}
 		}
 		return metaFileInfos, errs
@@ -1519,12 +1519,7 @@ func (er erasureObjects) putObject(ctx context.Context, bucket string, object st
 	onlineDisks, versions, oldDataDir, err := renameData(ctx, onlineDisks, minioMetaTmpBucket, tempObj, partsMetadata, bucket, object, writeQuorum)
 	if err != nil {
 		if errors.Is(err, errFileNotFound) {
-			// An in-quorum errFileNotFound means that client stream
-			// prematurely closed and we do not find any xl.meta or
-			// part.1's - in such a scenario we must return as if client
-			// disconnected. This means that erasure.Encode() CreateFile()
-			// did not do anything.
-			return ObjectInfo{}, IncompleteBody{Bucket: bucket, Object: object}
+			return ObjectInfo{}, toObjectErr(errErasureWriteQuorum, bucket, object)
 		}
 		return ObjectInfo{}, toObjectErr(err, bucket, object)
 	}
diff --git a/cmd/object-handlers_test.go b/cmd/object-handlers_test.go
index 0925c38c8..30529d58a 100644
--- a/cmd/object-handlers_test.go
+++ b/cmd/object-handlers_test.go
@@ -1237,7 +1237,6 @@ func testAPIPutObjectStreamSigV4Handler(obj ObjectLayer, instanceType, bucketNam
 		if err != nil {
 			t.Fatalf("Error injecting faults into the request: <ERROR> %v.", err)
 		}
-
 		// Since `apiRouter` satisfies `http.Handler` it has a ServeHTTP to execute the logic of the handler.
 		// Call the ServeHTTP to execute the handler,`func (api objectAPIHandlers) GetObjectHandler`  handles the request.
 		apiRouter.ServeHTTP(rec, req)
diff --git a/cmd/storage-rest-client.go b/cmd/storage-rest-client.go
index 294300774..3cd4dec9e 100644
--- a/cmd/storage-rest-client.go
+++ b/cmd/storage-rest-client.go
@@ -54,9 +54,6 @@ func isNetworkError(err error) bool {
 		if down := xnet.IsNetworkOrHostDown(nerr.Err, false); down {
 			return true
 		}
-		if errors.Is(nerr.Err, rest.ErrClientClosed) {
-			return true
-		}
 	}
 	if errors.Is(err, grid.ErrDisconnected) {
 		return true
@@ -64,7 +61,7 @@ func isNetworkError(err error) bool {
 	// More corner cases suitable for storage REST API
 	switch {
 	// A peer node can be in shut down phase and proactively
-	// return 503 server closed error, consider it as an offline node
+	// return 503 server closed error,consider it as an offline node
 	case strings.Contains(err.Error(), http.ErrServerClosed.Error()):
 		return true
 	// Corner case, the server closed the connection with a keep-alive timeout
diff --git a/cmd/storage-rest-server.go b/cmd/storage-rest-server.go
index a1387f5c0..ba829fc1b 100644
--- a/cmd/storage-rest-server.go
+++ b/cmd/storage-rest-server.go
@@ -791,26 +791,9 @@ func keepHTTPReqResponseAlive(w http.ResponseWriter, r *http.Request) (resp func
 		defer xioutil.SafeClose(doneCh)
 		// Initiate ticker after body has been read.
 		ticker := time.NewTicker(time.Second * 10)
-		defer ticker.Stop()
-
 		for {
 			select {
 			case <-ticker.C:
-				// The done() might have been called
-				// concurrently, check for it before we
-				// write the filler byte.
-				select {
-				case err := <-doneCh:
-					if err != nil {
-						write([]byte{1})
-						write([]byte(err.Error()))
-					} else {
-						write([]byte{0})
-					}
-					return
-				default:
-				}
-
 				// Response not ready, write a filler byte.
 				write([]byte{32})
 				if canWrite {
@@ -823,6 +806,7 @@ func keepHTTPReqResponseAlive(w http.ResponseWriter, r *http.Request) (resp func
 				} else {
 					write([]byte{0})
 				}
+				ticker.Stop()
 				return
 			}
 		}
@@ -870,21 +854,6 @@ func keepHTTPResponseAlive(w http.ResponseWriter) func(error) {
 		for {
 			select {
 			case <-ticker.C:
-				// The done() might have been called
-				// concurrently, check for it before we
-				// write the filler byte.
-				select {
-				case err := <-doneCh:
-					if err != nil {
-						write([]byte{1})
-						write([]byte(err.Error()))
-					} else {
-						write([]byte{0})
-					}
-					return
-				default:
-				}
-
 				// Response not ready, write a filler byte.
 				write([]byte{32})
 				if canWrite {
diff --git a/cmd/xl-storage-disk-id-check.go b/cmd/xl-storage-disk-id-check.go
index b987e5eb2..0e9d7aab9 100644
--- a/cmd/xl-storage-disk-id-check.go
+++ b/cmd/xl-storage-disk-id-check.go
@@ -909,8 +909,7 @@ func (p *xlStorageDiskIDCheck) monitorDiskStatus(spent time.Duration, fn string)
 		})
 
 		if err == nil {
-			logger.Event(context.Background(), "healthcheck",
-				"node(%s): Read/Write/Delete successful, bringing drive %s online", globalLocalNodeName, p.storage.String())
+			logger.Event(context.Background(), "node(%s): Read/Write/Delete successful, bringing drive %s online", globalLocalNodeName, p.storage.String())
 			p.health.status.Store(diskHealthOK)
 			p.health.waiting.Add(-1)
 			return
diff --git a/cmd/xl-storage.go b/cmd/xl-storage.go
index 1050a9e04..ee9815856 100644
--- a/cmd/xl-storage.go
+++ b/cmd/xl-storage.go
@@ -2558,12 +2558,8 @@ func (s *xlStorage) RenameData(ctx context.Context, srcVolume, srcPath string, f
 		}
 	}
 
-	// Preserve all the legacy data, could be slow, but at max there can be 10,000 parts.
-	currentDataPath := pathJoin(dstVolumeDir, dstPath)
-
 	var xlMeta xlMetaV2
 	var legacyPreserved bool
-	var legacyEntries []string
 	if len(dstBuf) > 0 {
 		if isXL2V1Format(dstBuf) {
 			if err = xlMeta.Load(dstBuf); err != nil {
@@ -2594,7 +2590,8 @@ func (s *xlStorage) RenameData(ctx context.Context, srcVolume, srcPath string, f
 			// from `xl.json` to `xl.meta`, we can avoid
 			// one extra readdir operation here for all
 			// new deployments.
-			entries, err := readDir(currentDataPath)
+			currentDataPath := pathJoin(dstVolumeDir, dstPath)
+			entries, err := readDirN(currentDataPath, 1)
 			if err != nil && err != errFileNotFound {
 				return res, osErrToFileErr(err)
 			}
@@ -2604,7 +2601,6 @@ func (s *xlStorage) RenameData(ctx context.Context, srcVolume, srcPath string, f
 				}
 				if strings.HasPrefix(entry, "part.") {
 					legacyPreserved = true
-					legacyEntries = entries
 					break
 				}
 			}
@@ -2615,29 +2611,31 @@ func (s *xlStorage) RenameData(ctx context.Context, srcVolume, srcPath string, f
 	if formatLegacy {
 		legacyDataPath = pathJoin(dstVolumeDir, dstPath, legacyDataDir)
 		if legacyPreserved {
-			if contextCanceled(ctx) {
-				return res, ctx.Err()
+			// Preserve all the legacy data, could be slow, but at max there can be 1res,000 parts.
+			currentDataPath := pathJoin(dstVolumeDir, dstPath)
+			entries, err := readDir(currentDataPath)
+			if err != nil {
+				return res, osErrToFileErr(err)
 			}
 
-			if len(legacyEntries) > 0 {
-				// legacy data dir means its old content, honor system umask.
-				if err = mkdirAll(legacyDataPath, 0o777, dstVolumeDir); err != nil {
-					// any failed mkdir-calls delete them.
-					s.deleteFile(dstVolumeDir, legacyDataPath, true, false)
-					return res, osErrToFileErr(err)
+			// legacy data dir means its old content, honor system umask.
+			if err = mkdirAll(legacyDataPath, 0o777, dstVolumeDir); err != nil {
+				// any failed mkdir-calls delete them.
+				s.deleteFile(dstVolumeDir, legacyDataPath, true, false)
+				return res, osErrToFileErr(err)
+			}
+
+			for _, entry := range entries {
+				// Skip xl.meta renames further, also ignore any directories such as `legacyDataDir`
+				if entry == xlStorageFormatFile || strings.HasSuffix(entry, slashSeparator) {
+					continue
 				}
-				for _, entry := range legacyEntries {
-					// Skip xl.meta renames further, also ignore any directories such as `legacyDataDir`
-					if entry == xlStorageFormatFile || strings.HasSuffix(entry, slashSeparator) {
-						continue
-					}
 
-					if err = Rename(pathJoin(currentDataPath, entry), pathJoin(legacyDataPath, entry)); err != nil {
-						// Any failed rename calls un-roll previous transaction.
-						s.deleteFile(dstVolumeDir, legacyDataPath, true, false)
+				if err = Rename(pathJoin(currentDataPath, entry), pathJoin(legacyDataPath, entry)); err != nil {
+					// Any failed rename calls un-roll previous transaction.
+					s.deleteFile(dstVolumeDir, legacyDataPath, true, false)
 
-						return res, osErrToFileErr(err)
-					}
+					return res, osErrToFileErr(err)
 				}
 			}
 		}
@@ -2728,10 +2726,6 @@ func (s *xlStorage) RenameData(ctx context.Context, srcVolume, srcPath string, f
 		return res, errFileCorrupt
 	}
 
-	if contextCanceled(ctx) {
-		return res, ctx.Err()
-	}
-
 	if err = s.WriteAll(ctx, srcVolume, pathJoin(srcPath, xlStorageFormatFile), newDstBuf); err != nil {
 		if legacyPreserved {
 			s.deleteFile(dstVolumeDir, legacyDataPath, true, false)
@@ -2755,9 +2749,6 @@ func (s *xlStorage) RenameData(ctx context.Context, srcVolume, srcPath string, f
 			// on a versioned bucket.
 			s.moveToTrash(legacyDataPath, true, false)
 		}
-		if contextCanceled(ctx) {
-			return res, ctx.Err()
-		}
 		if err = renameAll(srcDataPath, dstDataPath, skipParent); err != nil {
 			if legacyPreserved {
 				// Any failed rename calls un-roll previous transaction.
@@ -2767,16 +2758,11 @@ func (s *xlStorage) RenameData(ctx context.Context, srcVolume, srcPath string, f
 			s.deleteFile(dstVolumeDir, dstDataPath, false, false)
 			return res, osErrToFileErr(err)
 		}
-		diskHealthCheckOK(ctx, err)
 	}
 
 	// If we have oldDataDir then we must preserve current xl.meta
 	// as backup, in-case needing renames().
 	if res.OldDataDir != "" {
-		if contextCanceled(ctx) {
-			return res, ctx.Err()
-		}
-
 		// preserve current xl.meta inside the oldDataDir.
 		if err = s.writeAll(ctx, dstVolume, pathJoin(dstPath, res.OldDataDir, xlStorageFormatFileBackup), dstBuf, true, skipParent); err != nil {
 			if legacyPreserved {
@@ -2787,10 +2773,6 @@ func (s *xlStorage) RenameData(ctx context.Context, srcVolume, srcPath string, f
 		diskHealthCheckOK(ctx, err)
 	}
 
-	if contextCanceled(ctx) {
-		return res, ctx.Err()
-	}
-
 	// Commit meta-file
 	if err = renameAll(srcFilePath, dstFilePath, skipParent); err != nil {
 		if legacyPreserved {
diff --git a/internal/rest/client.go b/internal/rest/client.go
index 710d1ffe0..b143ce673 100644
--- a/internal/rest/client.go
+++ b/internal/rest/client.go
@@ -286,23 +286,12 @@ func (c *Client) dumpHTTP(req *http.Request, resp *http.Response) {
 	return
 }
 
-// ErrClientClosed returned when *Client is closed.
-var ErrClientClosed = errors.New("rest client is closed")
-
 // Call - make a REST call with context.
 func (c *Client) Call(ctx context.Context, method string, values url.Values, body io.Reader, length int64) (reply io.ReadCloser, err error) {
-	switch atomic.LoadInt32(&c.connected) {
-	case closed:
-		// client closed, this is usually a manual process
-		// so return a local error as client is closed
-		return nil, &NetworkError{Err: ErrClientClosed}
-	case offline:
-		// client offline, return last error captured.
+	if !c.IsOnline() {
 		return nil, &NetworkError{Err: c.LastError()}
 	}
 
-	// client is still connected, attempt the request.
-
 	// Shallow copy. We don't modify the *UserInfo, if set.
 	// All other fields are copied.
 	u := *c.url
@@ -404,6 +393,8 @@ func NewClient(uu *url.URL, tr http.RoundTripper, newAuthToken func(aud string)
 	clnt := &Client{
 		httpClient:               &http.Client{Transport: tr},
 		url:                      u,
+		lastErr:                  err,
+		lastErrTime:              time.Now(),
 		newAuthToken:             newAuthToken,
 		connected:                connected,
 		lastConn:                 time.Now().UnixNano(),
@@ -411,11 +402,6 @@ func NewClient(uu *url.URL, tr http.RoundTripper, newAuthToken func(aud string)
 		HealthCheckReconnectUnit: 200 * time.Millisecond,
 		HealthCheckTimeout:       time.Second,
 	}
-	if err != nil {
-		clnt.lastErr = err
-		clnt.lastErrTime = time.Now()
-	}
-
 	if clnt.HealthCheckFn != nil {
 		// make connection pre-emptively.
 		go clnt.HealthCheckFn()
@@ -482,7 +468,7 @@ func (c *Client) runHealthCheck() bool {
 					if atomic.CompareAndSwapInt32(&c.connected, offline, online) {
 						now := time.Now()
 						disconnected := now.Sub(c.LastConn())
-						logger.Event(context.Background(), "healthcheck", "Client '%s' re-connected in %s", c.url.String(), disconnected)
+						logger.Event(context.Background(), "Client '%s' re-connected in %s", c.url.String(), disconnected)
 						atomic.StoreInt64(&c.lastConn, now.UnixNano())
 					}
 					return
