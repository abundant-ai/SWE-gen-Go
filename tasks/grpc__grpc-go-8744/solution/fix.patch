diff --git a/internal/xds/resolver/xds_resolver.go b/internal/xds/resolver/xds_resolver.go
index 44b7dd4b..a38b292b 100644
--- a/internal/xds/resolver/xds_resolver.go
+++ b/internal/xds/resolver/xds_resolver.go
@@ -230,8 +230,12 @@ type xdsResolver struct {
 	curConfigSelector stoppableConfigSelector
 }
 
-// ResolveNow is a no-op at this point.
-func (*xdsResolver) ResolveNow(resolver.ResolveNowOptions) {}
+// ResolveNow calls RequestDNSReresolution on the dependency manager.
+func (r *xdsResolver) ResolveNow(opts resolver.ResolveNowOptions) {
+	if r.dm != nil {
+		r.dm.RequestDNSReresolution(opts)
+	}
+}
 
 func (r *xdsResolver) Close() {
 	// Cancel the context passed to the serializer and wait for any scheduled
diff --git a/internal/xds/xdsdepmgr/watch_service.go b/internal/xds/xdsdepmgr/watch_service.go
deleted file mode 100644
index 73f1b703..00000000
--- a/internal/xds/xdsdepmgr/watch_service.go
+++ /dev/null
@@ -1,83 +0,0 @@
-/*
- *
- * Copyright 2025 gRPC authors.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- *
- */
-
-package xdsdepmgr
-
-import "google.golang.org/grpc/internal/xds/xdsclient/xdsresource"
-
-type listenerWatcher struct {
-	resourceName string
-	cancel       func()
-	depMgr       *DependencyManager
-}
-
-func newListenerWatcher(resourceName string, depMgr *DependencyManager) *listenerWatcher {
-	lw := &listenerWatcher{resourceName: resourceName, depMgr: depMgr}
-	lw.cancel = xdsresource.WatchListener(depMgr.xdsClient, resourceName, lw)
-	return lw
-}
-
-func (l *listenerWatcher) ResourceChanged(update *xdsresource.ListenerUpdate, onDone func()) {
-	l.depMgr.onListenerResourceUpdate(update, onDone)
-}
-
-func (l *listenerWatcher) ResourceError(err error, onDone func()) {
-	l.depMgr.onListenerResourceError(err, onDone)
-}
-
-func (l *listenerWatcher) AmbientError(err error, onDone func()) {
-	l.depMgr.onListenerResourceAmbientError(err, onDone)
-}
-
-func (l *listenerWatcher) stop() {
-	l.cancel()
-	if l.depMgr.logger.V(2) {
-		l.depMgr.logger.Infof("Canceling watch on Listener resource %q", l.resourceName)
-	}
-}
-
-type routeConfigWatcher struct {
-	resourceName string
-	cancel       func()
-	depMgr       *DependencyManager
-}
-
-func newRouteConfigWatcher(resourceName string, depMgr *DependencyManager) *routeConfigWatcher {
-	rw := &routeConfigWatcher{resourceName: resourceName, depMgr: depMgr}
-	rw.cancel = xdsresource.WatchRouteConfig(depMgr.xdsClient, resourceName, rw)
-	return rw
-}
-
-func (r *routeConfigWatcher) ResourceChanged(u *xdsresource.RouteConfigUpdate, onDone func()) {
-	r.depMgr.onRouteConfigResourceUpdate(r.resourceName, u, onDone)
-}
-
-func (r *routeConfigWatcher) ResourceError(err error, onDone func()) {
-	r.depMgr.onRouteConfigResourceError(r.resourceName, err, onDone)
-}
-
-func (r *routeConfigWatcher) AmbientError(err error, onDone func()) {
-	r.depMgr.onRouteConfigResourceAmbientError(r.resourceName, err, onDone)
-}
-
-func (r *routeConfigWatcher) stop() {
-	r.cancel()
-	if r.depMgr.logger.V(2) {
-		r.depMgr.logger.Infof("Canceling watch on RouteConfiguration resource %q", r.resourceName)
-	}
-}
diff --git a/internal/xds/xdsdepmgr/xds_dependency_manager.go b/internal/xds/xdsdepmgr/xds_dependency_manager.go
index fae5ca96..266516bd 100644
--- a/internal/xds/xdsdepmgr/xds_dependency_manager.go
+++ b/internal/xds/xdsdepmgr/xds_dependency_manager.go
@@ -19,19 +19,28 @@
 package xdsdepmgr
 
 import (
+	"context"
 	"fmt"
+	"net/url"
 	"sync"
 
 	"google.golang.org/grpc/grpclog"
 	internalgrpclog "google.golang.org/grpc/internal/grpclog"
+	"google.golang.org/grpc/internal/grpcsync"
 	"google.golang.org/grpc/internal/xds/xdsclient"
 	"google.golang.org/grpc/internal/xds/xdsclient/xdsresource"
+	"google.golang.org/grpc/resolver"
+	"google.golang.org/grpc/serviceconfig"
 )
 
 const prefix = "[xdsdepmgr %p] "
 
 var logger = grpclog.Component("xds")
 
+// EnableClusterAndEndpointsWatch is a flag used to control whether the CDS/EDS
+// watchers in the dependency manager should be used.
+var EnableClusterAndEndpointsWatch = false
+
 func prefixLogger(p *DependencyManager) *internalgrpclog.PrefixLogger {
 	return internalgrpclog.NewPrefixLogger(logger, fmt.Sprintf(prefix, p))
 }
@@ -56,6 +65,40 @@ type ConfigWatcher interface {
 	Error(error)
 }
 
+// xdsResourceState is a generic struct to hold the state of a watched xDS
+// resource.
+type xdsResourceState[T any, U any] struct {
+	lastUpdate     *T
+	lastErr        error
+	updateReceived bool
+	stop           func()
+	extras         U // to store any additional state specific to the watcher
+}
+
+func (x *xdsResourceState[T, U]) setLastUpdate(update *T) {
+	x.lastUpdate = update
+	x.updateReceived = true
+	x.lastErr = nil
+}
+
+func (x *xdsResourceState[T, U]) setLastError(err error) {
+	x.lastErr = err
+	x.updateReceived = true
+	x.lastUpdate = nil
+}
+
+func (x *xdsResourceState[T, U]) updateLastError(err error) {
+	x.lastErr = err
+}
+
+type dnsExtras struct {
+	dnsR resolver.Resolver
+}
+
+type routeExtras struct {
+	virtualHost *xdsresource.VirtualHost
+}
+
 // DependencyManager registers watches on the xDS client for all required xDS
 // resources, resolves dependencies between them, and returns a complete
 // configuration to the xDS resolver.
@@ -69,16 +112,23 @@ type DependencyManager struct {
 	dataplaneAuthority string
 	nodeID             string
 
-	// All the fields below are protected by mu.
-	mu      sync.Mutex
-	stopped bool
+	// Used to serialize callbacks from DNS resolvers to avoid deadlocks. Since
+	// the resolver's Build() is called with the dependency manager lock held,
+	// direct callbacks to ClientConn (which also require that lock) would
+	// deadlock.
+	dnsSerializer       *grpcsync.CallbackSerializer
+	dnsSerializerCancel func()
 
-	listenerWatcher       *listenerWatcher
-	currentListenerUpdate *xdsresource.ListenerUpdate
-	routeConfigWatcher    *routeConfigWatcher
-	rdsResourceName       string
-	currentRouteConfig    *xdsresource.RouteConfigUpdate
-	currentVirtualHost    *xdsresource.VirtualHost
+	// All the fields below are protected by mu.
+	mu                      sync.Mutex
+	stopped                 bool
+	listenerWatcher         *xdsResourceState[xdsresource.ListenerUpdate, struct{}]
+	rdsResourceName         string
+	routeConfigWatcher      *xdsResourceState[xdsresource.RouteConfigUpdate, routeExtras]
+	clustersFromRouteConfig map[string]bool
+	clusterWatchers         map[string]*xdsResourceState[xdsresource.ClusterUpdate, struct{}]
+	endpointWatchers        map[string]*xdsResourceState[xdsresource.EndpointsUpdate, struct{}]
+	dnsResolvers            map[string]*xdsResourceState[xdsresource.DNSUpdate, dnsExtras]
 }
 
 // New creates a new DependencyManager.
@@ -91,18 +141,37 @@ type DependencyManager struct {
 //   - watcher is the ConfigWatcher interface that will receive the aggregated
 //     XDSConfig updates and errors.
 func New(listenerName, dataplaneAuthority string, xdsClient xdsclient.XDSClient, watcher ConfigWatcher) *DependencyManager {
+	ctx, cancel := context.WithCancel(context.Background())
 	dm := &DependencyManager{
-		ldsResourceName:    listenerName,
-		dataplaneAuthority: dataplaneAuthority,
-		xdsClient:          xdsClient,
-		watcher:            watcher,
-		nodeID:             xdsClient.BootstrapConfig().Node().GetId(),
+		ldsResourceName:         listenerName,
+		dataplaneAuthority:      dataplaneAuthority,
+		xdsClient:               xdsClient,
+		watcher:                 watcher,
+		nodeID:                  xdsClient.BootstrapConfig().Node().GetId(),
+		dnsSerializer:           grpcsync.NewCallbackSerializer(ctx),
+		dnsSerializerCancel:     cancel,
+		clustersFromRouteConfig: make(map[string]bool),
+		endpointWatchers:        make(map[string]*xdsResourceState[xdsresource.EndpointsUpdate, struct{}]),
+		dnsResolvers:            make(map[string]*xdsResourceState[xdsresource.DNSUpdate, dnsExtras]),
+		clusterWatchers:         make(map[string]*xdsResourceState[xdsresource.ClusterUpdate, struct{}]),
 	}
 	dm.logger = prefixLogger(dm)
 
 	// Start the listener watch. Listener watch will start the other resource
 	// watches as needed.
-	dm.listenerWatcher = newListenerWatcher(listenerName, dm)
+	dm.listenerWatcher = &xdsResourceState[xdsresource.ListenerUpdate, struct{}]{}
+	lw := &xdsResourceWatcher[xdsresource.ListenerUpdate]{
+		onUpdate: func(update *xdsresource.ListenerUpdate, onDone func()) {
+			dm.onListenerResourceUpdate(update, onDone)
+		},
+		onError: func(err error, onDone func()) {
+			dm.onListenerResourceError(err, onDone)
+		},
+		onAmbientError: func(err error, onDone func()) {
+			dm.onListenerResourceAmbientError(err, onDone)
+		},
+	}
+	dm.listenerWatcher.stop = xdsresource.WatchListener(dm.xdsClient, listenerName, lw)
 	return dm
 }
 
@@ -116,12 +185,27 @@ func (m *DependencyManager) Close() {
 	}
 
 	m.stopped = true
-	if m.listenerWatcher != nil {
-		m.listenerWatcher.stop()
-	}
+	m.listenerWatcher.stop()
 	if m.routeConfigWatcher != nil {
 		m.routeConfigWatcher.stop()
 	}
+	for name, cluster := range m.clusterWatchers {
+		cluster.stop()
+		delete(m.clusterWatchers, name)
+	}
+
+	for name, endpoint := range m.endpointWatchers {
+		endpoint.stop()
+		delete(m.endpointWatchers, name)
+	}
+
+	// We cannot wait for the dns serializer to finish here, as the callbacks
+	// try to grab the dependency manager lock, which is already held here.
+	m.dnsSerializerCancel()
+	for name, dnsResolver := range m.dnsResolvers {
+		dnsResolver.stop()
+		delete(m.dnsResolvers, name)
+	}
 }
 
 // annotateErrorWithNodeID annotates the given error with the provided xDS node
@@ -130,25 +214,263 @@ func (m *DependencyManager) annotateErrorWithNodeID(err error) error {
 	return fmt.Errorf("[xDS node id: %v]: %v", m.nodeID, err)
 }
 
-// maybeSendUpdateLocked checks that all the resources have been received and sends
-// the current aggregated xDS configuration to the watcher if all the updates
-// are available.
+// maybeSendUpdateLocked verifies that all expected resources have been
+// received, and if so, delivers the complete xDS configuration to the watcher.
 func (m *DependencyManager) maybeSendUpdateLocked() {
-	m.watcher.Update(&xdsresource.XDSConfig{
-		Listener:    m.currentListenerUpdate,
-		RouteConfig: m.currentRouteConfig,
-		VirtualHost: m.currentVirtualHost,
-	})
+	if m.listenerWatcher.lastUpdate == nil || m.routeConfigWatcher == nil || m.routeConfigWatcher.lastUpdate == nil {
+		return
+	}
+	config := &xdsresource.XDSConfig{
+		Listener:    m.listenerWatcher.lastUpdate,
+		RouteConfig: m.routeConfigWatcher.lastUpdate,
+		VirtualHost: m.routeConfigWatcher.extras.virtualHost,
+		Clusters:    make(map[string]*xdsresource.ClusterResult),
+	}
+
+	if !EnableClusterAndEndpointsWatch {
+		m.watcher.Update(config)
+		return
+	}
+
+	edsResourcesSeen := make(map[string]bool)
+	dnsResourcesSeen := make(map[string]bool)
+	clusterResourcesSeen := make(map[string]bool)
+	haveAllResources := true
+	for cluster := range m.clustersFromRouteConfig {
+		ok, leafClusters, err := m.populateClusterConfigLocked(cluster, 0, config.Clusters, edsResourcesSeen, dnsResourcesSeen, clusterResourcesSeen)
+		if !ok {
+			haveAllResources = false
+		}
+		// If there are no leaf clusters, add that as error.
+		if ok && len(leafClusters) == 0 {
+			config.Clusters[cluster] = &xdsresource.ClusterResult{Err: m.annotateErrorWithNodeID(fmt.Errorf("aggregate cluster graph has no leaf clusters"))}
+		}
+		if err != nil {
+			config.Clusters[cluster] = &xdsresource.ClusterResult{Err: err}
+		}
+	}
+
+	// Cancel resources not seen in the tree.
+	for name, ep := range m.endpointWatchers {
+		if _, ok := edsResourcesSeen[name]; !ok {
+			ep.stop()
+			delete(m.endpointWatchers, name)
+		}
+	}
+	for name, dr := range m.dnsResolvers {
+		if _, ok := dnsResourcesSeen[name]; !ok {
+			dr.stop()
+			delete(m.dnsResolvers, name)
+		}
+	}
+	for name, cluster := range m.clusterWatchers {
+		if _, ok := clusterResourcesSeen[name]; !ok {
+			cluster.stop()
+			delete(m.clusterWatchers, name)
+		}
+	}
+	if haveAllResources {
+		m.watcher.Update(config)
+	}
+}
+
+// populateClusterConfigLocked resolves and populates the
+// configuration for the given cluster and its children, including its
+// associated endpoint or aggregate children. For aggregate clusters, it
+// recursively resolves the configuration for its child clusters.
+//
+// This function traverses the cluster dependency graph (e.g., from an Aggregate
+// cluster down to its leaf clusters and their endpoints/DNS resources) to
+// ensure all necessary xDS resources are watched and fully resolved before
+// configuration is considered ready.
+//
+// Parameters:
+//
+//	clusterName:          The name of the cluster resource to resolve.
+//	depth:                The current recursion depth.
+//	clusterConfigs:       Map to store the resolved cluster configuration.
+//	endpointResourcesSeen: Stores which EDS resource names have been encountered.
+//	dnsResourcesSeen:      Stores which DNS resource names have been encountered.
+//	clustersSeen:         Stores which cluster resource names have been encountered.
+//
+// Returns:
+//
+//	bool:                 Returns true if the cluster configuration (and all its
+//	                      dependencies) is fully resolved (i.e either update or
+//	                      error has been received).
+//	[]string:             A slice of all "leaf" cluster names discovered in the
+//	                      traversal starting from `clusterName`. For
+//	                      non-aggregate clusters, this will contain only `clusterName`.
+//	error:                Error that needs to be propogated up the tree (like
+//	                      max depth exceeded or an error propagated from a
+//	                      child cluster).
+func (m *DependencyManager) populateClusterConfigLocked(clusterName string, depth int, clusterConfigs map[string]*xdsresource.ClusterResult, endpointResourcesSeen, dnsResourcesSeen, clustersSeen map[string]bool) (bool, []string, error) {
+	const aggregateClusterMaxDepth = 16
+	clustersSeen[clusterName] = true
+
+	if depth >= aggregateClusterMaxDepth {
+		err := m.annotateErrorWithNodeID(fmt.Errorf("aggregate cluster graph exceeds max depth (%d)", aggregateClusterMaxDepth))
+		clusterConfigs[clusterName] = &xdsresource.ClusterResult{Err: err}
+		return true, nil, err
+	}
+
+	// If cluster is already seen in the tree, return.
+	if _, ok := clusterConfigs[clusterName]; ok {
+		return true, nil, nil
+	}
+
+	// If cluster watcher does not exist, create one.
+	state, ok := m.clusterWatchers[clusterName]
+	if !ok {
+		m.clusterWatchers[clusterName] = newClusterWatcher(clusterName, m)
+		return false, nil, nil
+	}
+
+	// If a watch exists but no update received yet, return.
+	if !state.updateReceived {
+		return false, nil, nil
+	}
+
+	// If there was a resource error, propagate it up.
+	if state.lastErr != nil {
+		return true, nil, state.lastErr
+	}
+
+	clusterConfigs[clusterName] = &xdsresource.ClusterResult{
+		Config: xdsresource.ClusterConfig{
+			Cluster: state.lastUpdate,
+		},
+	}
+	update := state.lastUpdate
+
+	switch update.ClusterType {
+	case xdsresource.ClusterTypeEDS:
+		return m.populateEDSClusterLocked(clusterName, update, clusterConfigs, endpointResourcesSeen)
+	case xdsresource.ClusterTypeLogicalDNS:
+		return m.populateLogicalDNSClusterLocked(clusterName, update, clusterConfigs, dnsResourcesSeen)
+	case xdsresource.ClusterTypeAggregate:
+		return m.populateAggregateClusterLocked(clusterName, update, depth, clusterConfigs, endpointResourcesSeen, dnsResourcesSeen, clustersSeen)
+	default:
+		clusterConfigs[clusterName] = &xdsresource.ClusterResult{Err: m.annotateErrorWithNodeID(fmt.Errorf("cluster type %v of cluster %s not supported", update.ClusterType, clusterName))}
+		return true, nil, nil
+	}
+}
+
+func (m *DependencyManager) populateEDSClusterLocked(clusterName string, update *xdsresource.ClusterUpdate, clusterConfigs map[string]*xdsresource.ClusterResult, endpointResourcesSeen map[string]bool) (bool, []string, error) {
+	edsName := clusterName
+	if update.EDSServiceName != "" {
+		edsName = update.EDSServiceName
+	}
+	endpointResourcesSeen[edsName] = true
+
+	// If endpoint watcher does not exist, create one.
+	if _, ok := m.endpointWatchers[edsName]; !ok {
+		m.endpointWatchers[edsName] = newEndpointWatcher(edsName, m)
+		return false, nil, nil
+	}
+	endpointState := m.endpointWatchers[edsName]
+
+	// If the resource does not have any update yet, return.
+	if !endpointState.updateReceived {
+		return false, nil, nil
+	}
+
+	// Store the update and error.
+	clusterConfigs[clusterName].Config.EndpointConfig = &xdsresource.EndpointConfig{
+		EDSUpdate:      endpointState.lastUpdate,
+		ResolutionNote: endpointState.lastErr,
+	}
+	return true, []string{clusterName}, nil
+}
+
+func (m *DependencyManager) populateLogicalDNSClusterLocked(clusterName string, update *xdsresource.ClusterUpdate, clusterConfigs map[string]*xdsresource.ClusterResult, dnsResourcesSeen map[string]bool) (bool, []string, error) {
+	target := update.DNSHostName
+	dnsResourcesSeen[target] = true
+
+	// If dns resolver does not exist, create one.
+	if _, ok := m.dnsResolvers[target]; !ok {
+		state := m.newDNSResolver(target)
+		if state == nil {
+			return false, nil, nil
+		}
+		m.dnsResolvers[target] = state
+		return false, nil, nil
+	}
+	dnsState := m.dnsResolvers[target]
+
+	// If no update received, return false.
+	if !dnsState.updateReceived {
+		return false, nil, nil
+	}
+
+	clusterConfigs[clusterName].Config.EndpointConfig = &xdsresource.EndpointConfig{
+		DNSEndpoints:   dnsState.lastUpdate,
+		ResolutionNote: dnsState.lastErr,
+	}
+	return true, []string{clusterName}, nil
+}
+
+func (m *DependencyManager) populateAggregateClusterLocked(clusterName string, update *xdsresource.ClusterUpdate, depth int, clusterConfigs map[string]*xdsresource.ClusterResult, endpointResourcesSeen, dnsResourcesSeen, clustersSeen map[string]bool) (bool, []string, error) {
+	var leafClusters []string
+	haveAllResources := true
+	for _, child := range update.PrioritizedClusterNames {
+		ok, childLeafClusters, err := m.populateClusterConfigLocked(child, depth+1, clusterConfigs, endpointResourcesSeen, dnsResourcesSeen, clustersSeen)
+		if !ok {
+			haveAllResources = false
+		}
+		if err != nil {
+			clusterConfigs[clusterName] = &xdsresource.ClusterResult{Err: err}
+			return true, leafClusters, err
+		}
+		leafClusters = append(leafClusters, childLeafClusters...)
+	}
+	if !haveAllResources {
+		return false, leafClusters, nil
+	}
+	if haveAllResources && len(leafClusters) == 0 {
+		clusterConfigs[clusterName] = &xdsresource.ClusterResult{Err: m.annotateErrorWithNodeID(fmt.Errorf("aggregate cluster graph has no leaf clusters"))}
+		return true, leafClusters, nil
+	}
+	clusterConfigs[clusterName].Config.AggregateConfig = &xdsresource.AggregateConfig{
+		LeafClusters: leafClusters,
+	}
+	return true, leafClusters, nil
 }
 
 func (m *DependencyManager) applyRouteConfigUpdateLocked(update *xdsresource.RouteConfigUpdate) {
 	matchVH := xdsresource.FindBestMatchingVirtualHost(m.dataplaneAuthority, update.VirtualHosts)
 	if matchVH == nil {
-		m.watcher.Error(m.annotateErrorWithNodeID(fmt.Errorf("could not find VirtualHost for %q", m.dataplaneAuthority)))
+		err := m.annotateErrorWithNodeID(fmt.Errorf("could not find VirtualHost for %q", m.dataplaneAuthority))
+		m.routeConfigWatcher.setLastError(err)
+		m.watcher.Error(err)
 		return
 	}
-	m.currentRouteConfig = update
-	m.currentVirtualHost = matchVH
+	m.routeConfigWatcher.setLastUpdate(update)
+	m.routeConfigWatcher.extras.virtualHost = matchVH
+
+	if EnableClusterAndEndpointsWatch {
+		// Get the clusters to be watched from the routes in the virtual host.
+		// If the CLusterSpecifierField is set, we ignore it for now as the
+		// clusters will be determined dynamically for it.
+		newClusters := make(map[string]bool)
+
+		for _, rt := range matchVH.Routes {
+			for _, cluster := range rt.WeightedClusters {
+				newClusters[cluster.Name] = true
+			}
+		}
+		// Cancel watch for clusters not seen in route config
+		for name := range m.clustersFromRouteConfig {
+			if _, ok := newClusters[name]; !ok {
+				m.clusterWatchers[name].stop()
+				delete(m.clusterWatchers, name)
+			}
+		}
+
+		// Watch for new clusters is started in populateClusterConfigLocked to
+		// avoid repeating the code.
+		m.clustersFromRouteConfig = newClusters
+	}
 	m.maybeSendUpdateLocked()
 }
 
@@ -165,7 +487,7 @@ func (m *DependencyManager) onListenerResourceUpdate(update *xdsresource.Listene
 		m.logger.Infof("Received update for Listener resource %q: %+v", m.ldsResourceName, update)
 	}
 
-	m.currentListenerUpdate = update
+	m.listenerWatcher.setLastUpdate(update)
 
 	if update.InlineRouteConfig != nil {
 		// If there was a previous route config watcher because of a non-inline
@@ -173,8 +495,8 @@ func (m *DependencyManager) onListenerResourceUpdate(update *xdsresource.Listene
 		m.rdsResourceName = ""
 		if m.routeConfigWatcher != nil {
 			m.routeConfigWatcher.stop()
-			m.routeConfigWatcher = nil
 		}
+		m.routeConfigWatcher = &xdsResourceState[xdsresource.RouteConfigUpdate, routeExtras]{stop: func() {}}
 		m.applyRouteConfigUpdateLocked(update.InlineRouteConfig)
 		return
 	}
@@ -194,9 +516,25 @@ func (m *DependencyManager) onListenerResourceUpdate(update *xdsresource.Listene
 	m.rdsResourceName = update.RouteConfigName
 	if m.routeConfigWatcher != nil {
 		m.routeConfigWatcher.stop()
-		m.currentVirtualHost = nil
 	}
-	m.routeConfigWatcher = newRouteConfigWatcher(m.rdsResourceName, m)
+	rw := &xdsResourceWatcher[xdsresource.RouteConfigUpdate]{
+		onUpdate: func(update *xdsresource.RouteConfigUpdate, onDone func()) {
+			m.onRouteConfigResourceUpdate(m.rdsResourceName, update, onDone)
+		},
+		onError: func(err error, onDone func()) {
+			m.onRouteConfigResourceError(m.rdsResourceName, err, onDone)
+		},
+		onAmbientError: func(err error, onDone func()) {
+			m.onRouteConfigResourceAmbientError(m.rdsResourceName, err, onDone)
+		},
+	}
+	if m.routeConfigWatcher != nil {
+		m.routeConfigWatcher.stop = xdsresource.WatchRouteConfig(m.xdsClient, m.rdsResourceName, rw)
+	} else {
+		m.routeConfigWatcher = &xdsResourceState[xdsresource.RouteConfigUpdate, routeExtras]{
+			stop: xdsresource.WatchRouteConfig(m.xdsClient, m.rdsResourceName, rw),
+		}
+	}
 }
 
 func (m *DependencyManager) onListenerResourceError(err error, onDone func()) {
@@ -213,8 +551,8 @@ func (m *DependencyManager) onListenerResourceError(err error, onDone func()) {
 	if m.routeConfigWatcher != nil {
 		m.routeConfigWatcher.stop()
 	}
+	m.listenerWatcher.setLastError(err)
 	m.rdsResourceName = ""
-	m.currentVirtualHost = nil
 	m.routeConfigWatcher = nil
 	m.watcher.Error(fmt.Errorf("listener resource error: %v", m.annotateErrorWithNodeID(err)))
 }
@@ -258,6 +596,7 @@ func (m *DependencyManager) onRouteConfigResourceError(resourceName string, err
 	if m.stopped || m.rdsResourceName != resourceName {
 		return
 	}
+	m.routeConfigWatcher.setLastError(err)
 	m.logger.Warningf("Received resource error for RouteConfiguration resource %q: %v", resourceName, m.annotateErrorWithNodeID(err))
 	m.watcher.Error(fmt.Errorf("route resource error: %v", m.annotateErrorWithNodeID(err)))
 }
@@ -277,3 +616,268 @@ func (m *DependencyManager) onRouteConfigResourceAmbientError(resourceName strin
 
 	m.logger.Warningf("Route resource ambient error %q: %v", resourceName, m.annotateErrorWithNodeID(err))
 }
+
+func newClusterWatcher(resourceName string, depMgr *DependencyManager) *xdsResourceState[xdsresource.ClusterUpdate, struct{}] {
+	w := &xdsResourceWatcher[xdsresource.ClusterUpdate]{
+		onUpdate: func(u *xdsresource.ClusterUpdate, onDone func()) {
+			depMgr.onClusterResourceUpdate(resourceName, u, onDone)
+		},
+		onError: func(err error, onDone func()) {
+			depMgr.onClusterResourceError(resourceName, err, onDone)
+		},
+		onAmbientError: func(err error, onDone func()) {
+			depMgr.onClusterAmbientError(resourceName, err, onDone)
+		},
+	}
+	return &xdsResourceState[xdsresource.ClusterUpdate, struct{}]{
+		stop: xdsresource.WatchCluster(depMgr.xdsClient, resourceName, w),
+	}
+}
+
+// Records a successful Cluster resource update, clears any previous error.
+func (m *DependencyManager) onClusterResourceUpdate(resourceName string, update *xdsresource.ClusterUpdate, onDone func()) {
+	m.mu.Lock()
+	defer m.mu.Unlock()
+
+	defer onDone()
+	if m.stopped || m.clusterWatchers[resourceName] == nil {
+		return
+	}
+
+	if m.logger.V(2) {
+		m.logger.Infof("Received update for Cluster resource %q: %+v", resourceName, update)
+	}
+	m.clusterWatchers[resourceName].setLastUpdate(update)
+	m.maybeSendUpdateLocked()
+}
+
+// Records a resource error for a Cluster resource, clears the last successful
+// update since we want to stop using the resource if we get a resource error.
+func (m *DependencyManager) onClusterResourceError(resourceName string, err error, onDone func()) {
+	m.mu.Lock()
+	defer m.mu.Unlock()
+
+	defer onDone()
+	if m.stopped || m.clusterWatchers[resourceName] == nil {
+		return
+	}
+	m.logger.Warningf("Received resource error for Cluster resource %q: %v", resourceName, m.annotateErrorWithNodeID(err))
+	m.clusterWatchers[resourceName].setLastError(err)
+	m.maybeSendUpdateLocked()
+}
+
+// Records the error in the state. The last successful update is retained
+// because it should continue to be used as an amnbient error is received.
+func (m *DependencyManager) onClusterAmbientError(resourceName string, err error, onDone func()) {
+	m.mu.Lock()
+	defer m.mu.Unlock()
+
+	defer onDone()
+	if m.stopped || m.clusterWatchers[resourceName] == nil {
+		return
+	}
+	m.logger.Warningf("Cluster resource ambient error %q: %v", resourceName, m.annotateErrorWithNodeID(err))
+}
+
+func newEndpointWatcher(resourceName string, depMgr *DependencyManager) *xdsResourceState[xdsresource.EndpointsUpdate, struct{}] {
+	w := &xdsResourceWatcher[xdsresource.EndpointsUpdate]{
+		onUpdate: func(u *xdsresource.EndpointsUpdate, onDone func()) {
+			depMgr.onEndpointUpdate(resourceName, u, onDone)
+		},
+		onError: func(err error, onDone func()) {
+			depMgr.onEndpointResourceError(resourceName, err, onDone)
+		},
+		onAmbientError: func(err error, onDone func()) {
+			depMgr.onEndpointAmbientError(resourceName, err, onDone)
+		},
+	}
+	return &xdsResourceState[xdsresource.EndpointsUpdate, struct{}]{
+		stop: xdsresource.WatchEndpoints(depMgr.xdsClient, resourceName, w),
+	}
+}
+
+// Records a successful Endpoint resource update, clears any previous error from
+// the state.
+func (m *DependencyManager) onEndpointUpdate(resourceName string, update *xdsresource.EndpointsUpdate, onDone func()) {
+	m.mu.Lock()
+	defer m.mu.Unlock()
+
+	defer onDone()
+	if m.stopped || m.endpointWatchers[resourceName] == nil {
+		return
+	}
+
+	if m.logger.V(2) {
+		m.logger.Infof("Received update for Endpoint resource %q: %+v", resourceName, update)
+	}
+	m.endpointWatchers[resourceName].setLastUpdate(update)
+	m.maybeSendUpdateLocked()
+}
+
+// Records a resource error and clears the last successful update since the
+// endpoints should not be used after getting a resource error.
+func (m *DependencyManager) onEndpointResourceError(resourceName string, err error, onDone func()) {
+	m.mu.Lock()
+	defer m.mu.Unlock()
+
+	defer onDone()
+	if m.stopped || m.endpointWatchers[resourceName] == nil {
+		return
+	}
+	m.logger.Warningf("Received resource error for Endpoint resource %q: %v", resourceName, m.annotateErrorWithNodeID(err))
+	m.endpointWatchers[resourceName].setLastError(err)
+	m.maybeSendUpdateLocked()
+}
+
+// Records the ambient error without clearing the last successful update, as the
+// endpoints should continue to be used.
+func (m *DependencyManager) onEndpointAmbientError(resourceName string, err error, onDone func()) {
+	m.mu.Lock()
+	defer m.mu.Unlock()
+
+	defer onDone()
+	if m.stopped || m.endpointWatchers[resourceName] == nil {
+		return
+	}
+
+	m.logger.Warningf("Endpoint resource ambient error %q: %v", resourceName, m.annotateErrorWithNodeID(err))
+	m.endpointWatchers[resourceName].updateLastError(err)
+	m.maybeSendUpdateLocked()
+}
+
+// Converts the DNS resolver state to an internal update, handling address-only
+// updates by wrapping them into endpoints. It records the update and clears any
+// previous error.
+func (m *DependencyManager) onDNSUpdate(resourceName string, update *resolver.State) {
+	m.mu.Lock()
+	defer m.mu.Unlock()
+	if m.stopped || m.dnsResolvers[resourceName] == nil {
+		return
+	}
+
+	if m.logger.V(2) {
+		m.logger.Infof("Received update from DNS resolver for resource %q: %+v", resourceName, update)
+	}
+	var endpoints []resolver.Endpoint
+	if len(update.Endpoints) == 0 {
+		endpoints = make([]resolver.Endpoint, len(update.Addresses))
+		for i, a := range update.Addresses {
+			endpoints[i] = resolver.Endpoint{Addresses: []resolver.Address{a}}
+			endpoints[i].Attributes = a.BalancerAttributes
+		}
+	}
+
+	m.dnsResolvers[resourceName].setLastUpdate(&xdsresource.DNSUpdate{Endpoints: endpoints})
+	m.maybeSendUpdateLocked()
+}
+
+// Records a DNS resolver error. It clears the last update only if no successful
+// update has been received yet, then triggers a dependency update.
+//
+// If a previous good update was received, the error is recorded but the
+// previous update is retained for continued use. Errors are suppressed if a
+// resource error was already received, as further propagation would have no
+// downstream effect.
+func (m *DependencyManager) onDNSError(resourceName string, err error) {
+	m.mu.Lock()
+	defer m.mu.Unlock()
+
+	if m.stopped || m.dnsResolvers[resourceName] == nil {
+		return
+	}
+
+	err = fmt.Errorf("dns resolver error for target %q: %v", resourceName, m.annotateErrorWithNodeID(err))
+	m.logger.Warningf("%v", err)
+	state := m.dnsResolvers[resourceName]
+	if state.updateReceived {
+		state.updateLastError(err)
+		return
+	}
+
+	state.setLastError(err)
+	m.maybeSendUpdateLocked()
+}
+
+// RequestDNSReresolution calls all the the DNS resolver's ResolveNow.
+func (m *DependencyManager) RequestDNSReresolution(opt resolver.ResolveNowOptions) {
+	m.mu.Lock()
+	defer m.mu.Unlock()
+	for _, res := range m.dnsResolvers {
+		if res.extras.dnsR != nil {
+			res.extras.dnsR.ResolveNow(opt)
+		}
+	}
+}
+
+type resolverClientConn struct {
+	target string
+	depMgr *DependencyManager
+}
+
+func (rcc *resolverClientConn) UpdateState(state resolver.State) error {
+	rcc.depMgr.dnsSerializer.TrySchedule(func(context.Context) {
+		rcc.depMgr.onDNSUpdate(rcc.target, &state)
+	})
+	return nil
+}
+
+func (rcc *resolverClientConn) ReportError(err error) {
+	rcc.depMgr.dnsSerializer.TrySchedule(func(context.Context) {
+		rcc.depMgr.onDNSError(rcc.target, err)
+	})
+}
+
+func (rcc *resolverClientConn) NewAddress(addresses []resolver.Address) {
+	rcc.UpdateState(resolver.State{Addresses: addresses})
+}
+
+func (rcc *resolverClientConn) ParseServiceConfig(string) *serviceconfig.ParseResult {
+	return &serviceconfig.ParseResult{Err: fmt.Errorf("service config not supported")}
+}
+
+func (m *DependencyManager) newDNSResolver(target string) *xdsResourceState[xdsresource.DNSUpdate, dnsExtras] {
+	rcc := &resolverClientConn{
+		target: target,
+		depMgr: m,
+	}
+	u, err := url.Parse("dns:///" + target)
+	if err != nil {
+		err := fmt.Errorf("failed to parse DNS target %q: %v", target, m.annotateErrorWithNodeID(err))
+		m.logger.Warningf("%v", err)
+		rcc.ReportError(err)
+		return &xdsResourceState[xdsresource.DNSUpdate, dnsExtras]{}
+	}
+
+	r, err := resolver.Get("dns").Build(resolver.Target{URL: *u}, rcc, resolver.BuildOptions{})
+	if err != nil {
+		rcc.ReportError(err)
+		err := fmt.Errorf("failed to build DNS resolver for target %q: %v", target, m.annotateErrorWithNodeID(err))
+		m.logger.Warningf("%v", err)
+		return nil
+	}
+
+	return &xdsResourceState[xdsresource.DNSUpdate, dnsExtras]{
+		extras: dnsExtras{dnsR: r},
+		stop:   r.Close,
+	}
+}
+
+// xdsResourceWatcher is a generic implementation of the xdsresource.Watcher
+// interface.
+type xdsResourceWatcher[T any] struct {
+	onUpdate       func(*T, func())
+	onError        func(error, func())
+	onAmbientError func(error, func())
+}
+
+func (x *xdsResourceWatcher[T]) ResourceChanged(update *T, onDone func()) {
+	x.onUpdate(update, onDone)
+}
+
+func (x *xdsResourceWatcher[T]) ResourceError(err error, onDone func()) {
+	x.onError(err, onDone)
+}
+
+func (x *xdsResourceWatcher[T]) AmbientError(err error, onDone func()) {
+	x.onAmbientError(err, onDone)
+}
