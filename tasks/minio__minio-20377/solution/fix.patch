diff --git a/internal/config/notify/help.go b/internal/config/notify/help.go
index 4d07af917..343f46c7b 100644
--- a/internal/config/notify/help.go
+++ b/internal/config/notify/help.go
@@ -274,6 +274,18 @@ var (
 			Optional:    true,
 			Type:        "number",
 		},
+		config.HelpKV{
+			Key:         target.KafkaBatchSize,
+			Description: "batch size of the events; used only when queue_dir is set",
+			Optional:    true,
+			Type:        "number",
+		},
+		config.HelpKV{
+			Key:         target.KafkaBatchCommitTimeout,
+			Description: "commit timeout set for the batch; used only when batch_size > 1",
+			Optional:    true,
+			Type:        "duration",
+		},
 	}
 
 	HelpMQTT = config.HelpKVS{
diff --git a/internal/config/notify/parse.go b/internal/config/notify/parse.go
index 75091a67c..0f7dc4404 100644
--- a/internal/config/notify/parse.go
+++ b/internal/config/notify/parse.go
@@ -374,6 +374,10 @@ var (
 			Key:   target.KafkaBatchSize,
 			Value: "0",
 		},
+		config.KV{
+			Key:   target.KafkaBatchCommitTimeout,
+			Value: "0s",
+		},
 		config.KV{
 			Key:   target.KafkaCompressionCodec,
 			Value: "",
@@ -463,14 +467,23 @@ func GetNotifyKafka(kafkaKVS map[string]config.KVS) (map[string]target.KafkaArgs
 			return nil, err
 		}
 
+		batchCommitTimeoutEnv := target.EnvKafkaBatchCommitTimeout
+		if k != config.Default {
+			batchCommitTimeoutEnv = batchCommitTimeoutEnv + config.Default + k
+		}
+		batchCommitTimeout, err := time.ParseDuration(env.Get(batchCommitTimeoutEnv, kv.Get(target.KafkaBatchCommitTimeout)))
+		if err != nil {
+			return nil, err
+		}
 		kafkaArgs := target.KafkaArgs{
-			Enable:     enabled,
-			Brokers:    brokers,
-			Topic:      env.Get(topicEnv, kv.Get(target.KafkaTopic)),
-			QueueDir:   env.Get(queueDirEnv, kv.Get(target.KafkaQueueDir)),
-			QueueLimit: queueLimit,
-			Version:    env.Get(versionEnv, kv.Get(target.KafkaVersion)),
-			BatchSize:  uint32(batchSize),
+			Enable:             enabled,
+			Brokers:            brokers,
+			Topic:              env.Get(topicEnv, kv.Get(target.KafkaTopic)),
+			QueueDir:           env.Get(queueDirEnv, kv.Get(target.KafkaQueueDir)),
+			QueueLimit:         queueLimit,
+			Version:            env.Get(versionEnv, kv.Get(target.KafkaVersion)),
+			BatchSize:          uint32(batchSize),
+			BatchCommitTimeout: batchCommitTimeout,
 		}
 
 		tlsEnableEnv := target.EnvKafkaTLS
diff --git a/internal/store/batch.go b/internal/store/batch.go
index cba034f4b..bb31135c9 100644
--- a/internal/store/batch.go
+++ b/internal/store/batch.go
@@ -18,100 +18,123 @@
 package store
 
 import (
+	"context"
 	"errors"
-	"fmt"
 	"sync"
+	"time"
 )
 
 // ErrBatchFull indicates that the batch is full
 var ErrBatchFull = errors.New("batch is full")
 
-type key interface {
-	string | int | int64
-}
+const defaultCommitTimeout = 30 * time.Second
 
 // Batch represents an ordered batch
-type Batch[K key, T any] struct {
-	keys  []K
-	items map[K]T
-	limit uint32
+type Batch[I any] struct {
+	items  []I
+	limit  uint32
+	store  Store[I]
+	quitCh chan struct{}
 
 	sync.Mutex
 }
 
+// BatchConfig represents the batch config
+type BatchConfig[I any] struct {
+	Limit         uint32
+	Store         Store[I]
+	CommitTimeout time.Duration
+	Log           logger
+}
+
 // Add adds the item to the batch
-func (b *Batch[K, T]) Add(key K, item T) error {
+func (b *Batch[I]) Add(item I) error {
 	b.Lock()
 	defer b.Unlock()
 
 	if b.isFull() {
-		return ErrBatchFull
-	}
-
-	if _, ok := b.items[key]; !ok {
-		b.keys = append(b.keys, key)
+		if b.store == nil {
+			return ErrBatchFull
+		}
+		// commit batch to store
+		if err := b.commit(); err != nil {
+			return err
+		}
 	}
-	b.items[key] = item
 
+	b.items = append(b.items, item)
 	return nil
 }
 
-// GetAll fetches the items and resets the batch
-// Returned items are not referenced by the batch
-func (b *Batch[K, T]) GetAll() (orderedKeys []K, orderedItems []T, err error) {
+// Len returns the no of items in the batch
+func (b *Batch[_]) Len() int {
 	b.Lock()
 	defer b.Unlock()
 
-	orderedKeys = append([]K(nil), b.keys...)
-	for _, key := range orderedKeys {
-		item, ok := b.items[key]
-		if !ok {
-			err = fmt.Errorf("item not found for the key: %v; should not happen;", key)
-			return
-		}
-		orderedItems = append(orderedItems, item)
-		delete(b.items, key)
-	}
-
-	b.keys = b.keys[:0]
-
-	return
+	return len(b.items)
 }
 
-// GetByKey will get the batch item by the provided key
-func (b *Batch[K, T]) GetByKey(key K) (T, bool) {
-	b.Lock()
-	defer b.Unlock()
-
-	item, ok := b.items[key]
-	return item, ok
+func (b *Batch[_]) isFull() bool {
+	return len(b.items) >= int(b.limit)
 }
 
-// Len returns the no of items in the batch
-func (b *Batch[K, T]) Len() int {
-	b.Lock()
-	defer b.Unlock()
-
-	return len(b.keys)
+func (b *Batch[I]) commit() error {
+	switch len(b.items) {
+	case 0:
+		return nil
+	case 1:
+		_, err := b.store.Put(b.items[0])
+		return err
+	default:
+	}
+	if _, err := b.store.PutMultiple(b.items); err != nil {
+		return err
+	}
+	b.items = make([]I, 0, b.limit)
+	return nil
 }
 
-// IsFull checks if the batch is full or not
-func (b *Batch[K, T]) IsFull() bool {
+// Close commits the pending items and quits the goroutines
+func (b *Batch[I]) Close() error {
+	defer func() {
+		close(b.quitCh)
+	}()
+
 	b.Lock()
 	defer b.Unlock()
-
-	return b.isFull()
-}
-
-func (b *Batch[K, T]) isFull() bool {
-	return len(b.items) >= int(b.limit)
+	return b.commit()
 }
 
 // NewBatch creates a new batch
-func NewBatch[K key, T any](limit uint32) *Batch[K, T] {
-	return &Batch[K, T]{
-		keys:  make([]K, 0, limit),
-		items: make(map[K]T, limit),
-		limit: limit,
+func NewBatch[I any](config BatchConfig[I]) *Batch[I] {
+	if config.CommitTimeout == 0 {
+		config.CommitTimeout = defaultCommitTimeout
+	}
+	quitCh := make(chan struct{})
+	batch := &Batch[I]{
+		items:  make([]I, 0, config.Limit),
+		limit:  config.Limit,
+		store:  config.Store,
+		quitCh: quitCh,
+	}
+	if batch.store != nil {
+		go func() {
+			commitTicker := time.NewTicker(config.CommitTimeout)
+			defer commitTicker.Stop()
+			for {
+				select {
+				case <-commitTicker.C:
+				case <-batch.quitCh:
+					return
+				}
+				batch.Lock()
+				err := batch.commit()
+				batch.Unlock()
+				if err != nil {
+					config.Log(context.Background(), err, "")
+				}
+			}
+		}()
 	}
+	return batch
 }
diff --git a/internal/store/queuestore.go b/internal/store/queuestore.go
index 002202f49..1ee6278bb 100644
--- a/internal/store/queuestore.go
+++ b/internal/store/queuestore.go
@@ -18,24 +18,25 @@
 package store
 
 import (
+	"bytes"
 	"encoding/json"
 	"errors"
-	"fmt"
 	"os"
 	"path/filepath"
 	"sort"
-	"strings"
 	"sync"
 	"time"
 
 	"github.com/google/uuid"
 	jsoniter "github.com/json-iterator/go"
+	"github.com/klauspost/compress/s2"
 	"github.com/valyala/bytebufferpool"
 )
 
 const (
 	defaultLimit = 100000 // Default store limit.
 	defaultExt   = ".unknown"
+	compressExt  = ".snappy"
 )
 
 // errLimitExceeded error is sent when the maximum limit is reached.
@@ -83,18 +84,12 @@ func (store *QueueStore[_]) Open() error {
 		return err
 	}
 
-	// Truncate entries.
-	if uint64(len(files)) > store.entryLimit {
-		files = files[:store.entryLimit]
-	}
-
 	for _, file := range files {
 		if file.IsDir() {
 			continue
 		}
-		key := strings.TrimSuffix(file.Name(), store.fileExt)
 		if fi, err := file.Info(); err == nil {
-			store.entries[key] = fi.ModTime().UnixNano()
+			store.entries[file.Name()] = fi.ModTime().UnixNano()
 		}
 	}
 
@@ -107,96 +102,138 @@ func (store *QueueStore[_]) Delete() error {
 }
 
 // PutMultiple - puts an item to the store.
-func (store *QueueStore[I]) PutMultiple(item []I) error {
+func (store *QueueStore[I]) PutMultiple(items []I) (Key, error) {
 	// Generate a new UUID for the key.
-	key, err := uuid.NewRandom()
+	uid, err := uuid.NewRandom()
 	if err != nil {
-		return err
+		return Key{}, err
 	}
 
 	store.Lock()
 	defer store.Unlock()
 	if uint64(len(store.entries)) >= store.entryLimit {
-		return errLimitExceeded
+		return Key{}, errLimitExceeded
 	}
-	return store.multiWrite(fmt.Sprintf("%d:%s", len(item), key.String()), item)
+	key := Key{
+		Name:      uid.String(),
+		ItemCount: len(items),
+		Compress:  true,
+		Extension: store.fileExt,
+	}
+	return key, store.multiWrite(key, items)
 }
 
 // multiWrite - writes an item to the directory.
-func (store *QueueStore[I]) multiWrite(key string, item []I) error {
+func (store *QueueStore[I]) multiWrite(key Key, items []I) (err error) {
 	buf := bytebufferpool.Get()
 	defer bytebufferpool.Put(buf)
 
 	enc := jsoniter.ConfigCompatibleWithStandardLibrary.NewEncoder(buf)
 
-	for i := range item {
-		err := enc.Encode(item[i])
-		if err != nil {
+	for i := range items {
+		if err = enc.Encode(items[i]); err != nil {
 			return err
 		}
 	}
-	b := buf.Bytes()
 
-	path := filepath.Join(store.directory, key+store.fileExt)
-	err := os.WriteFile(path, b, os.FileMode(0o770))
+	path := filepath.Join(store.directory, key.String())
+	if key.Compress {
+		err = os.WriteFile(path, s2.Encode(nil, buf.Bytes()), os.FileMode(0o770))
+	} else {
+		err = os.WriteFile(path, buf.Bytes(), os.FileMode(0o770))
+	}
+
 	buf.Reset()
 	if err != nil {
 		return err
 	}
 
 	// Increment the item count.
-	store.entries[key] = time.Now().UnixNano()
+	store.entries[key.String()] = time.Now().UnixNano()
 
-	return nil
+	return
 }
 
 // write - writes an item to the directory.
-func (store *QueueStore[I]) write(key string, item I) error {
+func (store *QueueStore[I]) write(key Key, item I) error {
 	// Marshalls the item.
 	eventData, err := json.Marshal(item)
 	if err != nil {
 		return err
 	}
+	return store.writeBytes(key, eventData)
+}
 
-	path := filepath.Join(store.directory, key+store.fileExt)
-	if err := os.WriteFile(path, eventData, os.FileMode(0o770)); err != nil {
-		return err
+// writeBytes - writes bytes to the directory.
+func (store *QueueStore[I]) writeBytes(key Key, b []byte) (err error) {
+	path := filepath.Join(store.directory, key.String())
+
+	if key.Compress {
+		err = os.WriteFile(path, s2.Encode(nil, b), os.FileMode(0o770))
+	} else {
+		err = os.WriteFile(path, b, os.FileMode(0o770))
 	}
 
+	if err != nil {
+		return err
+	}
 	// Increment the item count.
-	store.entries[key] = time.Now().UnixNano()
-
+	store.entries[key.String()] = time.Now().UnixNano()
 	return nil
 }
 
 // Put - puts an item to the store.
-func (store *QueueStore[I]) Put(item I) error {
+func (store *QueueStore[I]) Put(item I) (Key, error) {
 	store.Lock()
 	defer store.Unlock()
 	if uint64(len(store.entries)) >= store.entryLimit {
-		return errLimitExceeded
+		return Key{}, errLimitExceeded
 	}
 	// Generate a new UUID for the key.
-	key, err := uuid.NewRandom()
+	uid, err := uuid.NewRandom()
 	if err != nil {
-		return err
+		return Key{}, err
 	}
-	return store.write(key.String(), item)
+	key := Key{
+		Name:      uid.String(),
+		Extension: store.fileExt,
+		ItemCount: 1,
+	}
+	return key, store.write(key, item)
+}
+
+// PutRaw - puts the raw bytes to the store
+func (store *QueueStore[I]) PutRaw(b []byte) (Key, error) {
+	store.Lock()
+	defer store.Unlock()
+	if uint64(len(store.entries)) >= store.entryLimit {
+		return Key{}, errLimitExceeded
+	}
+	// Generate a new UUID for the key.
+	uid, err := uuid.NewRandom()
+	if err != nil {
+		return Key{}, err
+	}
+	key := Key{
+		Name:      uid.String(),
+		Extension: store.fileExt,
+	}
+	return key, store.writeBytes(key, b)
 }
 
 // GetRaw - gets an item from the store.
-func (store *QueueStore[I]) GetRaw(key string) (raw []byte, err error) {
+func (store *QueueStore[I]) GetRaw(key Key) (raw []byte, err error) {
 	store.RLock()
 
 	defer func(store *QueueStore[I]) {
 		store.RUnlock()
-		if err != nil {
+		if err != nil && !os.IsNotExist(err) {
 			// Upon error we remove the entry.
 			store.Del(key)
 		}
 	}(store)
 
-	raw, err = os.ReadFile(filepath.Join(store.directory, key+store.fileExt))
+	raw, err = os.ReadFile(filepath.Join(store.directory, key.String()))
 	if err != nil {
 		return
 	}
@@ -209,19 +246,19 @@ func (store *QueueStore[I]) GetRaw(key string) (raw []byte, err error) {
 }
 
 // Get - gets an item from the store.
-func (store *QueueStore[I]) Get(key string) (item I, err error) {
+func (store *QueueStore[I]) Get(key Key) (item I, err error) {
 	store.RLock()
 
 	defer func(store *QueueStore[I]) {
 		store.RUnlock()
-		if err != nil {
+		if err != nil && !os.IsNotExist(err) {
 			// Upon error we remove the entry.
 			store.Del(key)
 		}
 	}(store)
 
 	var eventData []byte
-	eventData, err = os.ReadFile(filepath.Join(store.directory, key+store.fileExt))
+	eventData, err = os.ReadFile(filepath.Join(store.directory, key.String()))
 	if err != nil {
 		return item, err
 	}
@@ -237,26 +274,50 @@ func (store *QueueStore[I]) Get(key string) (item I, err error) {
 	return item, nil
 }
 
-// Del - Deletes an entry from the store.
-func (store *QueueStore[_]) Del(key string) error {
-	store.Lock()
-	defer store.Unlock()
-	return store.del(key)
-}
+// GetMultiple will read the multi payload file and fetch the items
+func (store *QueueStore[I]) GetMultiple(key Key) (items []I, err error) {
+	store.RLock()
 
-// DelList - Deletes a list of entries from the store.
-// Returns an error even if one key fails to be deleted.
-func (store *QueueStore[_]) DelList(keys []string) error {
-	store.Lock()
-	defer store.Unlock()
+	defer func(store *QueueStore[I]) {
+		store.RUnlock()
+		if err != nil && !os.IsNotExist(err) {
+			// Upon error we remove the entry.
+			store.Del(key)
+		}
+	}(store)
 
-	for _, key := range keys {
-		if err := store.del(key); err != nil {
-			return err
+	raw, err := os.ReadFile(filepath.Join(store.directory, key.String()))
+	if err != nil {
+		return
+	}
+
+	var decoder *jsoniter.Decoder
+	if key.Compress {
+		decodedBytes, err := s2.Decode(nil, raw)
+		if err != nil {
+			return nil, err
 		}
+		decoder = jsoniter.ConfigCompatibleWithStandardLibrary.NewDecoder(bytes.NewReader(decodedBytes))
+	} else {
+		decoder = jsoniter.ConfigCompatibleWithStandardLibrary.NewDecoder(bytes.NewReader(raw))
 	}
 
-	return nil
+	for decoder.More() {
+		var item I
+		if err := decoder.Decode(&item); err != nil {
+			return nil, err
+		}
+		items = append(items, item)
+	}
+
+	return
+}
+
+// Del - Deletes an entry from the store.
+func (store *QueueStore[_]) Del(key Key) error {
+	store.Lock()
+	defer store.Unlock()
+	return store.del(key)
 }
 
 // Len returns the entry count.
@@ -268,30 +329,35 @@ func (store *QueueStore[_]) Len() int {
 }
 
 // lockless call
-func (store *QueueStore[_]) del(key string) error {
-	err := os.Remove(filepath.Join(store.directory, key+store.fileExt))
+func (store *QueueStore[_]) del(key Key) error {
+	err := os.Remove(filepath.Join(store.directory, key.String()))
 
 	// Delete as entry no matter the result
-	delete(store.entries, key)
+	delete(store.entries, key.String())
 
 	return err
 }
 
 // List - lists all files registered in the store.
-func (store *QueueStore[_]) List() ([]string, error) {
+func (store *QueueStore[_]) List() (keys []Key) {
 	store.RLock()
-	l := make([]string, 0, len(store.entries))
-	for k := range store.entries {
-		l = append(l, k)
+	defer store.RUnlock()
+
+	entries := make([]string, 0, len(store.entries))
+	for entry := range store.entries {
+		entries = append(entries, entry)
 	}
 
 	// Sort entries...
-	sort.Slice(l, func(i, j int) bool {
-		return store.entries[l[i]] < store.entries[l[j]]
+	sort.Slice(entries, func(i, j int) bool {
+		return store.entries[entries[i]] < store.entries[entries[j]]
 	})
-	store.RUnlock()
 
-	return l, nil
+	for i := range entries {
+		keys = append(keys, parseKey(entries[i]))
+	}
+
+	return keys
 }
 
 // list will read all entries from disk.
@@ -318,9 +384,3 @@ func (store *QueueStore[_]) list() ([]os.DirEntry, error) {
 
 	return files, nil
 }
-
-// Extension will return the file extension used
-// for the items stored in the queue.
-func (store *QueueStore[_]) Extension() string {
-	return store.fileExt
-}
diff --git a/internal/store/store.go b/internal/store/store.go
index a72721856..fcd76dc60 100644
--- a/internal/store/store.go
+++ b/internal/store/store.go
@@ -21,6 +21,7 @@ import (
 	"context"
 	"errors"
 	"fmt"
+	"strconv"
 	"strings"
 	"time"
 
@@ -44,23 +45,64 @@ type Target interface {
 
 // Store - Used to persist items.
 type Store[I any] interface {
-	Put(item I) error
-	PutMultiple(item []I) error
-	Get(key string) (I, error)
-	GetRaw(key string) ([]byte, error)
+	Put(item I) (Key, error)
+	PutMultiple(item []I) (Key, error)
+	Get(key Key) (I, error)
+	GetMultiple(key Key) ([]I, error)
+	GetRaw(key Key) ([]byte, error)
+	PutRaw(b []byte) (Key, error)
 	Len() int
-	List() ([]string, error)
-	Del(key string) error
-	DelList(key []string) error
+	List() []Key
+	Del(key Key) error
 	Open() error
 	Delete() error
-	Extension() string
 }
 
 // Key denotes the key present in the store.
 type Key struct {
-	Name   string
-	IsLast bool
+	Name      string
+	Compress  bool
+	Extension string
+	ItemCount int
+}
+
+// String returns the filepath name
+func (k Key) String() string {
+	keyStr := k.Name
+	if k.ItemCount > 1 {
+		keyStr = fmt.Sprintf("%d:%s", k.ItemCount, k.Name)
+	}
+	return keyStr + k.Extension + func() string {
+		if k.Compress {
+			return compressExt
+		}
+		return ""
+	}()
+}
+
+func getItemCount(k string) (count int, err error) {
+	count = 1
+	v := strings.Split(k, ":")
+	if len(v) == 2 {
+		return strconv.Atoi(v[0])
+	}
+	return
+}
+
+func parseKey(k string) (key Key) {
+	key.Name = k
+	if strings.HasSuffix(k, compressExt) {
+		key.Compress = true
+		key.Name = strings.TrimSuffix(key.Name, compressExt)
+	}
+	if key.ItemCount, _ = getItemCount(k); key.ItemCount > 1 {
+		key.Name = strings.TrimPrefix(key.Name, fmt.Sprintf("%d:", key.ItemCount))
+	}
+	if vals := strings.Split(key.Name, "."); len(vals) == 2 {
+		key.Extension = "." + vals[1]
+		key.Name = strings.TrimSuffix(key.Name, key.Extension)
+	}
+	return
 }
 
 // replayItems - Reads the items from the store and replays.
@@ -74,18 +116,12 @@ func replayItems[I any](store Store[I], doneCh <-chan struct{}, log logger, id s
 		defer retryTicker.Stop()
 
 		for {
-			names, err := store.List()
-			if err != nil {
-				log(context.Background(), fmt.Errorf("store.List() failed with: %w", err), id)
-			} else {
-				keyCount := len(names)
-				for i, name := range names {
-					select {
-					case keyCh <- Key{strings.TrimSuffix(name, store.Extension()), keyCount == i+1}:
-					// Get next key.
-					case <-doneCh:
-						return
-					}
+			for _, key := range store.List() {
+				select {
+				case keyCh <- key:
+				// Get next key.
+				case <-doneCh:
+					return
 				}
 			}
 
@@ -114,7 +150,7 @@ func sendItems(target Target, keyCh <-chan Key, doneCh <-chan struct{}, logger l
 
 			logger(
 				context.Background(),
-				fmt.Errorf("unable to send webhook log entry to '%s' err '%w'", target.Name(), err),
+				fmt.Errorf("unable to send log entry to '%s' err '%w'", target.Name(), err),
 				target.Name(),
 			)
 
